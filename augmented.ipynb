{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Synthesis using Generative Adversarial Network (Not Used)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x000001F784196180>\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Load dataset normal\n",
    "df = pd.read_csv(\"SP1_new.csv\")\n",
    "\n",
    "# Pilih fitur yang akan digunakan untuk sintesis data fault\n",
    "features = ['Output Frequency', 'Output Current', 'Output Power', 'Fin Temperature', 'Cooling Fin Temperature']\n",
    "data = df[features].values\n",
    "\n",
    "# Normalisasi data agar sesuai untuk Neural Network\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "data_normalized = scaler.fit_transform(data)\n",
    "\n",
    "# Konversi ke Tensor\n",
    "data_tensor = torch.FloatTensor(data_normalized)\n",
    "dataset = DataLoader(data_tensor, batch_size=64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, output_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.model(z)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diskriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid() \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model and Optimizer Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter model\n",
    "input_dim = 5  # Jumlah fitur\n",
    "latent_dim = 10  # Dimensi noise (input ke Generator)\n",
    "lr = 0.0002  # Learning rate\n",
    "\n",
    "# Inisialisasi model\n",
    "generator = Generator(latent_dim, input_dim)\n",
    "discriminator = Discriminator(input_dim)\n",
    "\n",
    "# Optimizer\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=lr)\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=lr)\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.BCELoss()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/5000] | Loss_D: 1.3516 | Loss_G: 0.5986\n",
      "Epoch [500/5000] | Loss_D: 1.0097 | Loss_G: 1.1501\n",
      "Epoch [1000/5000] | Loss_D: 1.1532 | Loss_G: 0.8698\n",
      "Epoch [1500/5000] | Loss_D: 1.5330 | Loss_G: 0.6314\n",
      "Epoch [2000/5000] | Loss_D: 1.3269 | Loss_G: 0.6982\n",
      "Epoch [2500/5000] | Loss_D: 1.0819 | Loss_G: 0.9170\n",
      "Epoch [3000/5000] | Loss_D: 0.8628 | Loss_G: 1.4938\n",
      "Epoch [3500/5000] | Loss_D: 1.0117 | Loss_G: 1.6976\n",
      "Epoch [4000/5000] | Loss_D: 0.7498 | Loss_G: 1.3781\n",
      "Epoch [4500/5000] | Loss_D: 0.9112 | Loss_G: 1.2428\n",
      "Training GAN selesai!\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "num_epochs = 5000\n",
    "batch_size = 64\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for real_data in dataset:\n",
    "        batch_size = real_data.size(0)\n",
    "        \n",
    "        # ======== TRAIN DISCRIMINATOR ========\n",
    "        real_labels = torch.ones(batch_size, 1)  # Label 1 untuk data asli\n",
    "        fake_labels = torch.zeros(batch_size, 1)  # Label 0 untuk data buatan\n",
    "        \n",
    "        optimizer_D.zero_grad()\n",
    "        \n",
    "        # Evaluasi data asli\n",
    "        real_output = discriminator(real_data)\n",
    "        loss_real = criterion(real_output, real_labels)\n",
    "        \n",
    "        # Evaluasi data sintetik\n",
    "        noise = torch.randn(batch_size, latent_dim)\n",
    "        fake_data = generator(noise)\n",
    "        fake_output = discriminator(fake_data.detach())\n",
    "        loss_fake = criterion(fake_output, fake_labels)\n",
    "        \n",
    "        # Total loss Discriminator\n",
    "        loss_D = loss_real + loss_fake\n",
    "        loss_D.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # ======== TRAIN GENERATOR ========\n",
    "        optimizer_G.zero_grad()\n",
    "        \n",
    "        fake_output = discriminator(fake_data)\n",
    "        loss_G = criterion(fake_output, real_labels)  # Generator ingin fool Discriminator\n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "    if epoch % 500 == 0:\n",
    "        print(f\"Epoch [{epoch}/{num_epochs}] | Loss_D: {loss_D:.4f} | Loss_G: {loss_G:.4f}\")\n",
    "\n",
    "print(\"Training GAN selesai!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Synthesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data fault sintetis berhasil dibuat dan disimpan!\n"
     ]
    }
   ],
   "source": [
    "# Buat 1000 sampel data fault sintetis\n",
    "num_samples = 1000\n",
    "noise = torch.randn(num_samples, latent_dim)\n",
    "synthetic_fault_data = generator(noise).detach().numpy()\n",
    "\n",
    "# Transform back ke skala asli\n",
    "synthetic_fault_data = scaler.inverse_transform(synthetic_fault_data)\n",
    "\n",
    "# Simpan ke CSV\n",
    "df_synthetic = pd.DataFrame(synthetic_fault_data, columns=features)\n",
    "df_synthetic.to_csv(\"data_vfd_fault_synthetic.csv\", index=False)\n",
    "\n",
    "print(\"Data fault sintetis berhasil dibuat dan disimpan!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
